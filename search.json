[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Project Title",
    "section": "",
    "text": "Your report goes here.\n\n\nCode\n# a test code chunk",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "eda/eda-member1.html",
    "href": "eda/eda-member1.html",
    "title": "Member 1 DK",
    "section": "",
    "text": "Your exploratory data analysis of the team datasets go here.\n\n\nCode\nlibrary(readr)\natp_tennis_raw &lt;- read_csv(\"../data/processed/atp_tennis.csv\")\n\n\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"tidymodels\")\n\n\n\n# 1. DATA PREPARATION, FEATURE ENGINEERING, AND MODEL SPECIFICATION (CONSOLIDATED)\n# Goal: Create the outcome and select non-obvious features (Points and Context)\natp_tennis &lt;- atp_tennis_raw %&gt;%\n  # Filter step added to include ONLY matches where Nadal R. is playing\n  filter(Player_1 == \"Nadal R.\" | Player_2 == \"Nadal R.\") %&gt;%\n  mutate(\n    P1_Wins = ifelse(Winner == Player_1, \"Yes\", \"No\") %&gt;% factor(levels = c(\"Yes\", \"No\"))\n  ) %&gt;%\n  # Filter out matches where we don't have basic data (using Pts_1 as a proxy)\n  filter(Pts_1 != -1) %&gt;%\n  drop_na() %&gt;%\n  # Select ONLY the non-obvious features as requested (Points and Context)\n  select(\n    P1_Wins,\n    Surface, Round, Series, Court\n  )\n\n# Define the LASSO model specification\n# We use tune() to signal that the optimal penalty must be found later.\nlasso_spec &lt;- logistic_reg() %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_args(mixture = 1, penalty = tune())\n\n\n# 2. RECIPE, WORKFLOW, TUNING GRID (CONSOLIDATED)\n# Define the preprocessing steps\n# NOTE: step_normalize is ESSENTIAL for LASSO\nvariable_recipe &lt;- recipe(P1_Wins ~ ., data = atp_tennis) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())\n\n# Combine model and recipe into a workflow\nlasso_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(lasso_spec)\n\n# Set up tuning parameters: 20-fold CV and a grid of 20 penalty values\nset.seed(253)\n# NOTE: With a smaller, specialized dataset, you might want fewer folds or repeats.\n# We will keep v=20 for consistency but be aware it's taxing on small datasets.\ncv_folds &lt;- vfold_cv(atp_tennis, v = 10, strata = P1_Wins)\nlasso_grid &lt;- grid_regular(penalty(range = c(-5, 0)), levels = 10) \n\n# 3. CROSS-VALIDATION AND SELECTION (CONSOLIDATED)\n# Run the tuning process\nlasso_models &lt;- lasso_workflow %&gt;% \n  tune_grid(\n    grid = lasso_grid,\n    resamples = cv_folds,\n    metrics = metric_set(roc_auc) \n  )\n\n# Select the best penalty and finalize\nbest_penalty &lt;- lasso_models %&gt;%\n  select_best(metric = \"roc_auc\")\n\nfinal_lasso_workflow &lt;- lasso_workflow %&gt;%\n  finalize_workflow(best_penalty)\n\n# Output Results\ncat(\"\\n\\n--- Tuning Complete ---\\n\")\n\n\n\n\n--- Tuning Complete ---\n\n\nCode\ncat(\"The optimal penalty parameter (lambda) that maximizes the ROC AUC is:\\n\")\n\n\nThe optimal penalty parameter (lambda) that maximizes the ROC AUC is:\n\n\nCode\nprint(best_penalty)\n\n\n# A tibble: 1 × 2\n  penalty .config         \n    &lt;dbl&gt; &lt;chr&gt;           \n1  0.0774 pre0_mod08_post0\n\n\nCode\ncat(\"\\n\\n--- Final Workflow Ready ---\\n\")\n\n\n\n\n--- Final Workflow Ready ---\n\n\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"tidymodels\")\n# Load required library for broom's tidy() on glmnet models\nlibrary(\"glmnet\") \n\n# --- ASSUMPTION ---\n# This script assumes that 'atp_tennis' data, 'lasso_workflow', and 'best_penalty'\n# have been successfully created by the 'lasso_tuning.R' script.\n\n# --- 1. Finalize Workflow with the Best Penalty ---\n# This replaces 'penalty = tune()' with the optimal value found during tuning.\nfinal_lasso_workflow &lt;- lasso_workflow %&gt;%\n  finalize_workflow(best_penalty)\n\ncat(\"\\n\\n--- Fitting Final Model ---\\n\")\n\n\n\n\n--- Fitting Final Model ---\n\n\nCode\n# --- 2. Fit the Final Model to the Entire Dataset ---\n# This step trains the optimized LASSO model on the full preprocessed data.\n# NOTE: This model relies ONLY on Player Points (Pts_1, Pts_2) and Match Context.\nfinal_fit &lt;- fit(final_lasso_workflow, data = atp_tennis)\n\n# --- 3. Inspect Model Coefficients ---\n# Use 'extract_fit_engine' to get the underlying glmnet object, \n# and 'tidy' to see the coefficients at the chosen penalty level.\n\n# 3a. Extract the underlying glmnet fit\nglmnet_fit &lt;- final_fit %&gt;%\n  extract_fit_engine()\n\n# 3b. Extract the specific penalty value\nlambda_best &lt;- best_penalty$penalty\n\n# 3c. Print the model coefficients for the optimal lambda\n# The tidy function shows which variables were kept and which were shrunk to 0.\nmodel_coefficients &lt;- tidy(glmnet_fit, s = lambda_best) %&gt;%\n  # Filter out coefficients that were set exactly to zero by the LASSO penalty\n  filter(estimate != 0) %&gt;%\n  arrange(desc(abs(estimate)))\n\ncat(\"\\n\\n--- Key Model Coefficients (Excluding Zeroed Variables) ---\\n\")\n\n\n\n\n--- Key Model Coefficients (Excluding Zeroed Variables) ---\n\n\nCode\nprint(model_coefficients)\n\n\n# A tibble: 506 × 5\n   term                  step estimate   lambda dev.ratio\n   &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Series_International    40    0.510 0.000610   0.00898\n 2 Series_International    39    0.506 0.000670   0.00897\n 3 Series_International    38    0.501 0.000735   0.00895\n 4 Series_International    37    0.497 0.000807   0.00894\n 5 Series_International    36    0.493 0.000885   0.00893\n 6 Series_International    35    0.491 0.000972   0.00891\n 7 Series_International    34    0.488 0.00107    0.00890\n 8 Series_International    33    0.485 0.00117    0.00889\n 9 Series_International    32    0.481 0.00128    0.00888\n10 Series_International    31    0.478 0.00141    0.00886\n# ℹ 496 more rows\n\n\nCode\ncat(\"\\n\\n--- Model Interpretation Notes ---\\n\")\n\n\n\n\n--- Model Interpretation Notes ---\n\n\nCode\ncat(\"This model ignores Rank and Odds. Interpretation now centers on:\\n\")\n\n\nThis model ignores Rank and Odds. Interpretation now centers on:\n\n\nCode\ncat(\"1. Player Points (Pts_1, Pts_2): A higher Pts_1 estimate should positively impact P1's chances.\\n\")\n\n\n1. Player Points (Pts_1, Pts_2): A higher Pts_1 estimate should positively impact P1's chances.\n\n\nCode\ncat(\"2. Context: The coefficients for Surface, Round, Series, and Court show which specific match conditions favor P1.\\n\")\n\n\n2. Context: The coefficients for Surface, Round, Series, and Court show which specific match conditions favor P1.\n\n\nCode\ncat(\"Variables with positive coefficients increase the probability of Player 1 winning.\\n\")\n\n\nVariables with positive coefficients increase the probability of Player 1 winning.\n\n\nCode\ncat(\"Variables not listed here were shrunk to zero by the LASSO penalty (lambda = \", lambda_best, \").\\n\")\n\n\nVariables not listed here were shrunk to zero by the LASSO penalty (lambda =  0.07742637 ).\n\n\n\nThis is a dataset that all of the ATP Dataset. This provides a lot of information about all of the matches from the past 25 years. We decided to ask the question of what are the most important factors in a player winning a set. Do certain players always preform better on a certain tennis surface, or part of a tournament. Is there one specific person that always loses to one player more than normal. I have decided to use machine learning to help me evaluate this database. I used a lasso model, which will tell us the most important variables in finding whether there is a win or not. I decided to just focus on one player Rafael Nadal, because I think that it is a lot more readable. But One of the major takeaways that I have gained from this data analysis is that there is not a lot of an impact of the type of court or surface, but there is a bit of a correlation between the series. It appears Nadal preforms better in the international series compared to the Grand Slam Series. But I also see that this estimate is still too low to be able to draw meaningful conclusions from this analysis.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Member 1 DK</span>"
    ]
  },
  {
    "objectID": "eda/eda-member2.html",
    "href": "eda/eda-member2.html",
    "title": "Member 2 First Name & Last Name Initial",
    "section": "",
    "text": "Your exploratory data analysis of the team datasets go here.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Member 2 First Name & Last Name Initial</span>"
    ]
  },
  {
    "objectID": "eda/eda-member3.html",
    "href": "eda/eda-member3.html",
    "title": "Member 3 First Name & Last Name Initial",
    "section": "",
    "text": "Your exploratory data analysis of the team datasets go here.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Member 3 First Name & Last Name Initial</span>"
    ]
  },
  {
    "objectID": "eda/PM4.html",
    "href": "eda/PM4.html",
    "title": "EDA",
    "section": "",
    "text": "Code\ninstall.packages(\"DataExplorer\")\n\n\nThe following package(s) will be installed:\n- DataExplorer [0.8.4]\nThese packages will be installed into \"~/Documents/GitHub/project-kangyu-dooley/renv/library/macos/R-4.5/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing DataExplorer ...                   OK [linked from cache]\nSuccessfully installed 1 package in 5.5 milliseconds.\n\n\n\n\nCode\nlibrary(readr)\natp_tennis &lt;- read_csv(\"../data/processed/atp_tennis.csv\")\nlibrary(DataExplorer)\ncreate_report(atp_tennis)\n\n\n\n  |                                           \n  |                                     |   0%\n  |                                           \n  |.                                    |   2%                                 \n  |                                           \n  |..                                   |   5% [global_options]                \n  |                                           \n  |...                                  |   7%                                 \n  |                                           \n  |....                                 |  10% [introduce]                     \n  |                                           \n  |....                                 |  12%                                 \n  |                                           \n  |.....                                |  14% [plot_intro]                    \n\n\n\n  |                                           \n  |......                               |  17%                                 \n  |                                           \n  |.......                              |  19% [data_structure]                \n  |                                           \n  |........                             |  21%                                 \n  |                                           \n  |.........                            |  24% [missing_profile]               \n\n\n\n  |                                           \n  |..........                           |  26%                                 \n  |                                           \n  |...........                          |  29% [univariate_distribution_header]\n  |                                           \n  |...........                          |  31%                                 \n  |                                           \n  |............                         |  33% [plot_histogram]                \n\n\n\n  |                                           \n  |.............                        |  36%                                 \n  |                                           \n  |..............                       |  38% [plot_density]                  \n  |                                           \n  |...............                      |  40%                                 \n  |                                           \n  |................                     |  43% [plot_frequency_bar]            \n\n\n\n  |                                           \n  |.................                    |  45%                                 \n  |                                           \n  |..................                   |  48% [plot_response_bar]             \n  |                                           \n  |..................                   |  50%                                 \n  |                                           \n  |...................                  |  52% [plot_with_bar]                 \n  |                                           \n  |....................                 |  55%                                 \n  |                                           \n  |.....................                |  57% [plot_normal_qq]                \n\n\n\n  |                                           \n  |......................               |  60%                                 \n  |                                           \n  |.......................              |  62% [plot_response_qq]              \n  |                                           \n  |........................             |  64%                                 \n  |                                           \n  |.........................            |  67% [plot_by_qq]                    \n  |                                           \n  |..........................           |  69%                                 \n  |                                           \n  |..........................           |  71% [correlation_analysis]          \n\n\n\n  |                                           \n  |...........................          |  74%                                 \n  |                                           \n  |............................         |  76% [principal_component_analysis]  \n\n\n\n  |                                           \n  |.............................        |  79%                                 \n  |                                           \n  |..............................       |  81% [bivariate_distribution_header] \n  |                                           \n  |...............................      |  83%                                 \n  |                                           \n  |................................     |  86% [plot_response_boxplot]         \n  |                                           \n  |.................................    |  88%                                 \n  |                                           \n  |.................................    |  90% [plot_by_boxplot]               \n  |                                           \n  |..................................   |  93%                                 \n  |                                           \n  |...................................  |  95% [plot_response_scatterplot]     \n  |                                           \n  |.................................... |  98%                                 \n  |                                           \n  |.....................................| 100% [plot_by_scatterplot]           \n                                                                                                                           \n\n\n/private/var/folders/x_/my1plq49665d3y56ckzspp2m0000gp/T/AppTranslocation/542A6D63-257F-4F96-8038-CE2164280B8A/d/RStudio.app/Contents/Resources/app/quarto/bin/tools/aarch64/pandoc +RTS -K512m -RTS /Users/Dooley/Documents/GitHub/project-kangyu-dooley/eda/report.knit.md --to html4 --from markdown+autolink_bare_uris+tex_math_single_backslash --output /Users/Dooley/Documents/GitHub/project-kangyu-dooley/eda/report.html --lua-filter /Users/Dooley/Library/Caches/org.R-project.R/R/renv/cache/v5/macos/R-4.5/aarch64-apple-darwin20/rmarkdown/2.30/efe19db0fde0fff13cea7eec6f695021/rmarkdown/rmarkdown/lua/pagebreak.lua --lua-filter /Users/Dooley/Library/Caches/org.R-project.R/R/renv/cache/v5/macos/R-4.5/aarch64-apple-darwin20/rmarkdown/2.30/efe19db0fde0fff13cea7eec6f695021/rmarkdown/rmarkdown/lua/latex-div.lua --lua-filter /Users/Dooley/Library/Caches/org.R-project.R/R/renv/cache/v5/macos/R-4.5/aarch64-apple-darwin20/rmarkdown/2.30/efe19db0fde0fff13cea7eec6f695021/rmarkdown/rmarkdown/lua/table-classes.lua --embed-resources --standalone --variable bs3=TRUE --section-divs --table-of-contents --toc-depth 6 --template /Users/Dooley/Library/Caches/org.R-project.R/R/renv/cache/v5/macos/R-4.5/aarch64-apple-darwin20/rmarkdown/2.30/efe19db0fde0fff13cea7eec6f695021/rmarkdown/rmd/h/default.html --no-highlight --variable highlightjs=1 --variable theme=yeti --mathjax --variable 'mathjax-url=https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML' --include-in-header /var/folders/x_/my1plq49665d3y56ckzspp2m0000gp/T//Rtmpa2v7dI/rmarkdown-str11e7a7b1ac387.html \n\n\n\nThe ATP dataset is exceptionally clean and well-structured, comprising 66,613 match records across 17 variables without any missing values, which ensures robust downstream analysis. Categorical variables like Tournament, Player_1/2, and Score exhibit high cardinality, so they’re best encoded carefully or excluded from certain models. Key distributions reveal that most matches are best-of-3 played outdoors on hard courts in ATP250 and Grand Slam events, while Masters and finals (best-of-5) are less frequent. Correlation and PCA diagnostics highlight strong within-feature coherence and various positive/negative associations between performance metrics (Pts_1, Pts_2) and betting odds (Odd_1, Odd_2), suggesting these numerical features capture complementary aspects of match competitiveness. Overall, the dataset’s completeness and varied feature types make it ideal for predictive modeling of match outcomes, ranking impact studies, or betting odds analysis.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>EDA</span>"
    ]
  },
  {
    "objectID": "appx/proposal.html",
    "href": "appx/proposal.html",
    "title": "Appendix A — Proposal",
    "section": "",
    "text": "Code\nlibrary(knitr)\n\n\nProject Proposal: Analyzing the Evolution of Men’s Professional Tennis (2000-2023)\nTeam Members: Dooley Kim, Kangyu Li\nShort Project Description\nThis project aims to perform a comprehensive data analysis of the Association of Tennis Professionals(https://www.atptour.com/en/corporate/about) Tour match results spanning over two decades (2000–2023). Our analysis will focus on how performance metrics have shifted, what factors contribute most significantly to match victory across different court surfaces, and the feasibility of building predictive models for match outcomes.\nResearch Questions (2-3 Broad Questions) How have crucial match stats evolved across different court surfaces between the pre-2010 and post-2010 era?\nWhat are the most significant predictors of a match victory, and do they vary between major tournaments and standard tour events?\nCan a machine learning model reliably predict the winner of an ATP match with a high degree of accuracy?\nReasons and Inspirations The inspiration for this project stems from a shared interest in sports analytics. The dataset provides a unique opportunity to study a long-term, high-quality athletic phenomenon. We aim to move beyond simple win-loss records to understand the underlying statistical mechanics that drive success in modern professional tennis, making the analysis relevant to both sports enthusiasts and data science practitioners.\nProject Implementation Milestone 3 We will work together on studying and evaluating our case study\nMilestone 4 We will each do our own analysis and when finished, meet up to compare what we have found\nMilestone 5 Findings will be divided equally among the group members and each member will be responsible for presenting their share of the findings.\nMilestone 6 Different graphs annd parts of the datastory will be divided amoungst the group members equally. Each person will be responsible for their section and reviewing another member’s section.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Proposal</span>"
    ]
  },
  {
    "objectID": "appx/case-study.html",
    "href": "appx/case-study.html",
    "title": "Appendix B — Case Study",
    "section": "",
    "text": "Source: https://pudding.cool/2017/02/new-slang/\nIntroduction:\nThis case study explores the origins and spread of slang, examining how language dynamically evolves through cultural influences like hip-hop, politics, and the internet. Using interactive visualizations and geographic analysis, it traces the rise and fall of popular slang terms, revealing how digital platforms and regional trends shape the way we communicate.\nThe report chooses a very fascinating topic: Where slang comes from. It is attractive as it reflects how language constantly evolves with culture and social media. New slang terms spread quickly online, helping people express identity, humor, and belonging in creative ways.\nThe visualizations made in this report are worth digging and learnable. For example, the first visualization uses a multi-line time series plot structure. The design effectively shows rise-and-fall trends of multiple slang terms simultaneously. The use of distinct colors and clear word labels allows readers to easily follow individual trends. You can click the filter words to check how their trends changed on the plot accordingly. This is a clear first step that they filter all the interesting slang words. The next step is to create heat maps and small multiple structures to show geographic and temporal variation in search interest for each slang word. Some slangs are popular in specific states, these terms more or less stem from local events or the influence of viral celebrities. The whole report is clearly organized, with three big catalysts part: Hip hop with slang as the title, Politics, and Internets. By combining these effective visualization techniques: interactive line charts for temporal trends, heat maps for geographic context, the report successfully transforms raw search data into an insightful exploration of language evolution. The authors also point out the limitation of the research, they honestly state they were unable to find the causes for the popularity of three words.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Case Study</span>"
    ]
  },
  {
    "objectID": "wa/wa-member1.html",
    "href": "wa/wa-member1.html",
    "title": "Appendix C — Member 1 First Name & Last Name Initial",
    "section": "",
    "text": "Work area to try code",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Member 1 First Name & Last Name Initial</span>"
    ]
  },
  {
    "objectID": "wa/wa-member2.html",
    "href": "wa/wa-member2.html",
    "title": "Appendix D — Member 2 First Name & Last Name Initial",
    "section": "",
    "text": "Work area to try code",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Member 2 First Name & Last Name Initial</span>"
    ]
  },
  {
    "objectID": "wa/wa-member3.html",
    "href": "wa/wa-member3.html",
    "title": "Appendix E — Member 3 First Name & Last Name Initial",
    "section": "",
    "text": "Work area to try code",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Member 3 First Name & Last Name Initial</span>"
    ]
  }
]